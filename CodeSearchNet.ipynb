{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CodeSearchNet Data Source Notice"
      ],
      "metadata": {
        "id": "d3QskDTbwoUM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsyNUB9POBaT",
        "outputId": "b5012ffa-877e-4a49-9b8e-23baeb968616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/CodeSearchNet’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/CodeSearchNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install docopt"
      ],
      "metadata": {
        "id": "C5yUBaR0SA32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the CodeSearchNet dataset was archieved, the S3 bucket was taken offline. As a result, following the installation on the github installation guide will not work. A short illustration can be seen down below."
      ],
      "metadata": {
        "id": "C2p5D1KzWk_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from subprocess import call, check_call, CalledProcessError\n",
        "\n",
        "destination_dir = \"/content/CodeSearchNet\"\n",
        "\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "os.chdir(destination_dir)\n",
        "\n",
        "try:\n",
        "    language = \"python\"\n",
        "    check_call(['wget', f'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/{language}.zip', '-O', f'{language}.zip'])\n",
        "    check_call(['unzip', f'{language}.zip'])\n",
        "    check_call(['rm', f'{language}.zip'])\n",
        "except CalledProcessError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(f\"Error executing command {e.cmd}\")\n",
        "    print(f\"Returned code {e.returncode}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgBuEGUIOyB1",
        "outputId": "2eb1de01-f530-4fb2-c1c6-1c93d8e33699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['wget', 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip', '-O', 'python.zip']' returned non-zero exit status 8.\n",
            "Error executing command ['wget', 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip', '-O', 'python.zip']\n",
            "Returned code 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead, we download the dataset from Hugging Face. Updating `datasets` might not be necessary but might sometimes be helpful to avoid errors concering caching in the local file system"
      ],
      "metadata": {
        "id": "OC_ULT-kXAoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Fetching"
      ],
      "metadata": {
        "id": "yzT8WmsywReH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "fbljfl0sJDen"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"code_search_net\", \"python\")\n",
        "\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]\n",
        "validation_data = dataset[\"validation\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "e53d5b5e18ab4401aec332af8898327f",
            "75c5d498660946dd87fb0df910470e77",
            "26224f25a0a948ed84355b95766b0593",
            "290bb8685837473484c58227d308e3e0",
            "72aa9bd9941047cea6e42cbb8c9f610d",
            "05c4088795f54fa695a47240ff8d0f4a",
            "b1a1bbe153d14d33b87c97cb9bc49ae5",
            "1d9160898f894897871f06e38feb009e",
            "21b2c72351e64be4aadb817282187357",
            "cc099e3173994a6290b9291b783b6c47",
            "f65743214c5541c0b203dbea6b8566a1",
            "717db61c3e2e41e3aa171e1482cd19f6",
            "3169d296be0f417f9fdf7aeeed4ae2c6",
            "e2d17d0af2a94645b15185b76b3b9281",
            "5398ec7737934ecc9b8f058fe4447115",
            "a0c7cd983f594e31b70db3f63af8d9a8",
            "cc302f0573424b17b58ed3ffbdcfa230",
            "dfd7c76113484335a9c158c90ccb0deb",
            "2fad7f6b5dab4e37a264a47921bb9218",
            "b408c2b2e2a14999b8dab4c789a07c68",
            "185e4aa5fe0d48118a64b92a5d2bd05f",
            "6395e8c1e4c2486fab702f485fa7fd59",
            "679104a4941848e5bfaff6fd1e3e97c5",
            "145f55a3f1b24903bc609369657a432f",
            "39cbbeb9f3554dceac30b9551640a223",
            "e4c09c86be774ec7b39be9cd101035ab",
            "9e16b591fd2642ba84dd6d0c48165531",
            "84c424f2f71644e8bbf73feb762fb408",
            "8de48b080e084cccbb5dfc5f7b9dbe55",
            "01bdabd979c44d6faf3221cc864b52d3",
            "a2973b5321ad4cbaa8bc7aab0f311dde",
            "f73b90448b31432b91c3712f98e33422",
            "01edb208a7914826a70ecf25c55832c0",
            "64c3e342bda042798bf9a710c8c25dd9",
            "9daddbf2313743f4b6c01c87c10ede25",
            "4adbde6e5031453198538088a54edec3",
            "2208d81494aa4b669e45ae139cdf1ee5",
            "e377e8bd4df64cb086e3bdd82fdf02a0",
            "11a9f27a81b942d1a6ba816085e73656",
            "2fcf46009dc345268eb24ac6e98df7da",
            "7af6a6b9746642bc98d65f76a2b54d74",
            "bafe659cfbab40be839dfef6c93c1bae",
            "c4abb97668174991bc59f9185432a030",
            "97d16bb7736e47ef9520ced175189f73",
            "8e5c52c3fb5341468e1f35ddb21ac0df",
            "f195558496c04f6392d8078c7a202e76",
            "ff14479da2854c179a2019245df70931",
            "85d7865a08dc4535948524570f018371",
            "14b2d6dd1b5e44a9ab4e4d5eb3a16abe",
            "4e1e8a699b3f4af0947baadb0d7cead5",
            "0667c47c5ba944b4acc9de7155ca9571",
            "0b8697144aa24360aa76099b86ff3ff5",
            "9d6ade8c50164854b9b3628e767093ee",
            "428df433a93846a48ae030144572b12f",
            "12f41d9da0ee400cb7c08be8b94a367d",
            "ec8d5d30b3894aa29c6ecba4dd29704e",
            "3dc617e4df99466fb4b15e07bd6e8e3f",
            "e02c63304c7040bb920ae42c02cf3c33",
            "7aacfa0a55f14bf290e9241c2fc4c0bf",
            "c0681b8707cb403d89739318284606f6",
            "dba5624e217c4acca5cacb7490dcce82",
            "7ceace4c5ec84b71bcc5474dd9990046",
            "671db084d92c4213bf8c108e72070845",
            "a496b45e494b431ab9f30f88aa326761",
            "3f977d4bb7864a5ea8a7e6e275b8374c",
            "05d8811ef7634b498f85194340f92f4b"
          ]
        },
        "id": "5MfdVWN1JELf",
        "outputId": "61a78d94-b125-4d12-b8f4-9b8261ca43a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e53d5b5e18ab4401aec332af8898327f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "code_search_net.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "717db61c3e2e41e3aa171e1482cd19f6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "python.zip:   0%|          | 0.00/941M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "679104a4941848e5bfaff6fd1e3e97c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64c3e342bda042798bf9a710c8c25dd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e5c52c3fb5341468e1f35ddb21ac0df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec8d5d30b3894aa29c6ecba4dd29704e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect the contents of the dataset object for the training, testing, and validation datasets."
      ],
      "metadata": {
        "id": "FQfMMFc3b6vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.features.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCTJR_45byNv",
        "outputId": "cc6e23d4-5cb0-4799-9973-5d1475c00a6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Generation Pipeline"
      ],
      "metadata": {
        "id": "Ssi7fBcnweX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer\n",
        ")\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "from typing import (\n",
        "    Tuple,\n",
        "    Dict,\n",
        "    Literal,\n",
        "    List,\n",
        "    Generator,\n",
        "    overload,\n",
        "    Union\n",
        ")\n",
        "\n",
        "class QGPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str,\n",
        "        qg_format: Literal[\"highlight\"] = \"highlight\",\n",
        "        exclude_after: List[str] = [],\n",
        "        use_cuda: bool = False\n",
        "    ):\n",
        "\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        self.qg_format = qg_format\n",
        "\n",
        "        assert self.model.__class__.__name__ == \"T5ForConditionalGeneration\"\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.use_cuda = use_cuda\n",
        "        self._exclude_after = exclude_after\n",
        "        print(f\"Using {self.device}\")\n",
        "\n",
        "    def __call__(self, input: Union[Tuple[str, str], List[Tuple[str, str]]]):\n",
        "        if isinstance(input, tuple):\n",
        "            # Handle single input\n",
        "            func_name, docstring = input\n",
        "            questions = self._generate_questions(func_name, docstring)\n",
        "            output = [{'answer': func_name, 'question': que} for que in questions]\n",
        "            if output:\n",
        "                 return output[0]\n",
        "            else:\n",
        "                 return {}\n",
        "\n",
        "        elif isinstance(input, list) and all(isinstance(item, tuple) for item in input):\n",
        "            # Handle batch input with proper error handling\n",
        "            return self._process_batch_generator(input)\n",
        "        else:\n",
        "            raise TypeError(\"Invalid input type. Expected a tuple (func_name, docstring) or a list of such tuples.\")\n",
        "\n",
        "\n",
        "    def _process_batch_generator(self, batch_input: List[Tuple[str, str]]) -> Generator[Dict[str, Any], None, None]:\n",
        "        \"\"\"\n",
        "        Process batch input and yield results with error handling per item\n",
        "        \"\"\"\n",
        "        for i, (func_name, docstring) in enumerate(batch_input):\n",
        "            try:\n",
        "                questions = self._generate_questions(func_name, docstring)\n",
        "                output = [{'answer': func_name, 'question': que} for que in questions]\n",
        "\n",
        "                if output:\n",
        "                    yield {\n",
        "                        'success': True,\n",
        "                        'index': i,\n",
        "                        'function_name': func_name,\n",
        "                        'docstring': docstring,\n",
        "                        'result': output[0],\n",
        "                        'error': None\n",
        "                    }\n",
        "                else:\n",
        "                    yield {\n",
        "                        'success': False,\n",
        "                        'index': i,\n",
        "                        'function_name': func_name,\n",
        "                        'docstring': docstring,\n",
        "                        'result': {},\n",
        "                        'error': 'No questions generated'\n",
        "                    }\n",
        "\n",
        "            except Exception as e:\n",
        "                yield {\n",
        "                    'success': False,\n",
        "                    'index': i,\n",
        "                    'function_name': func_name,\n",
        "                    'docstring': docstring,\n",
        "                    'result': {},\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "    def _generate_questions(self, func_name, docstring):\n",
        "        #TODO: This can be re-written in a more forceful way for the llm\n",
        "        inputs = self._prepare_inputs_for_question_extraction(func_name, docstring)\n",
        "\n",
        "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outs = self.model.generate(\n",
        "                input_ids=inputs['input_ids'].to(self.device),\n",
        "                attention_mask=inputs['attention_mask'].to(self.device),\n",
        "                num_beams=4,\n",
        "\n",
        "                max_length=32\n",
        "            )\n",
        "\n",
        "        questions = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _tokenize(self, inputs, padding=True, truncation=True, add_special_tokens=True, max_length=512):\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return tokenized_inputs\n",
        "\n",
        "    def _prepare_inputs_for_question_extraction(self, func_name, docstring):\n",
        "        #NOTE: experimental, consider removing :params and :return values\n",
        "        #manual observation suggests the model struggles to understand the pupose of the function in their presense\n",
        "        for string in self._exclude_after:\n",
        "            param_idx = docstring.find(string)\n",
        "            if param_idx != -1:\n",
        "                docstring = docstring[:param_idx]\n",
        "            docstring = docstring.strip()\n",
        "        input = f\"answer: <hl>The function is {func_name}<hl>. Context: {docstring} </s>\"\n",
        "\n",
        "        return [input]\n",
        "\n",
        "    @property\n",
        "    def exclude_after(self):\n",
        "        return self._exclude_after\n",
        "\n",
        "    @exclude_after.setter\n",
        "    def exclude_after(self, value):\n",
        "        self._exclude_after = value"
      ],
      "metadata": {
        "id": "0jlDE7JajLqk"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_t5 = QGPipeline(model=\"valhalla/t5-base-qg-hl\")"
      ],
      "metadata": {
        "id": "b0DxchuaFNVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f3157d-5f7b-42c8-f059-477614744ada"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func_name = train_data[4]['func_name']\n",
        "docstring = train_data[4]['func_documentation_string']\n",
        "print(docstring)\n",
        "finetuned_t5((func_name, docstring))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w2QjHhVyhdy",
        "outputId": "e1f7c529-03e1-43bc-cc02-404053eb314a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parses the XML run statistics file (GenerateFASTQRunStatistics.xml). In some cases, the file is not\n",
            "        available. Equivalent data can be pulled from Basespace.Generate a text file  name indexingQC.txt containing\n",
            "        the copied tables from the Indexing QC tab of the run on Basespace\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Metadata.parserunstats',\n",
              " 'question': 'What is the function that parses the XML run statistics file?'}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result looks promising. Let's run the model for the first 20 doc strings in our dataset"
      ],
      "metadata": {
        "id": "PllIvQOqRx_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    func_name = train_data[i]['func_name']\n",
        "    p = docstring = train_data[i]['func_documentation_string']\n",
        "    print(f\"========Sample{i+1}==========\")\n",
        "    idx = docstring.find(\":param\")\n",
        "    if idx != -1:\n",
        "        p = docstring[:idx]\n",
        "    p = docstring[:].strip()\n",
        "    print(f\"Docstring: {docstring}\")\n",
        "    print(finetuned_t5(func_name, docstring))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XB0O656n6lzA",
        "outputId": "db5c1a05-c661-4a6b-9afe-57854f3d99c8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========Sample1==========\n",
            "Docstring: Display slug with level by language.\n",
            "[{'answer': 'show_slug_with_level', 'question': 'What is the name of the function that displays a slug with level by language?'}]\n",
            "========Sample2==========\n",
            "Docstring: Render the last 10 revisions of a page content with a list using\n",
            "        the ``pages/revisions.html`` template\n",
            "[{'answer': 'show_revisions', 'question': 'What is the name of the function that shows the last 10 revisions of a page?'}]\n",
            "========Sample3==========\n",
            "Docstring: Method that parse the imageplaceholder template tag.\n",
            "[{'answer': 'do_videoplaceholder', 'question': 'What is the name of the method that parses the imageplaceholder template tag?'}]\n",
            "========Sample4==========\n",
            "Docstring: Return Pages with given tag\n",
            "\n",
            "    Syntax::\n",
            "\n",
            "        {% get_pages_with_tag <tag name> as <varname> %}\n",
            "\n",
            "    Example use:\n",
            "        {% get_pages_with_tag \"footer\" as pages %}\n",
            "[{'answer': 'do_get_pages_with_tag', 'question': 'What is the name of the function that returns pages with given tag?'}]\n",
            "========Sample5==========\n",
            "Docstring: Parses the XML run statistics file (GenerateFASTQRunStatistics.xml). In some cases, the file is not\n",
            "        available. Equivalent data can be pulled from Basespace.Generate a text file  name indexingQC.txt containing\n",
            "        the copied tables from the Indexing QC tab of the run on Basespace\n",
            "[{'answer': 'Metadata.parserunstats', 'question': 'What is the name of the file that parses the XML run statistics file?'}]\n",
            "========Sample6==========\n",
            "Docstring: Prettify name of path\n",
            "\n",
            "    :param path: path to fix\n",
            "    :return: Good name for path\n",
            "[{'answer': 'fix_raw_path', 'question': 'What is the name of the file that is used to fix a path?'}]\n",
            "========Sample7==========\n",
            "Docstring: Removes year from input\n",
            "\n",
            "    :param name: path to edit\n",
            "    :return: inputs with no years\n",
            "[{'answer': 'remove_year', 'question': 'What is the name of the function that removes a year from input?'}]\n",
            "========Sample8==========\n",
            "Docstring: Removes brackets form input\n",
            "\n",
            "    :param name: path to fix\n",
            "    :return: inputs with no brackets\n",
            "[{'answer': 'remove_brackets', 'question': 'What is the name of the function that removes brackets from input?'}]\n",
            "========Sample9==========\n",
            "Docstring: Extracts max chars in name truncated to nearest word\n",
            "\n",
            "    :param name: path to edit\n",
            "    :param max_chars: max chars of new name\n",
            "    :param blank: char that represents the blank between words\n",
            "    :return: Name edited to contain at most max_chars\n",
            "[{'answer': 'extract_name_max_chars', 'question': 'What is the name of the function that extracts maximum chars in name?'}]\n",
            "========Sample10==========\n",
            "Docstring: Prettify name of path\n",
            "\n",
            "    :param name: path Name: to edit\n",
            "    :param blank: default blanks in name\n",
            "    :return: Prettier name from given one: replace bad chars with good ones\n",
            "[{'answer': 'prettify', 'question': 'What is the name of the path that is used to make it easier to find?'}]\n",
            "========Sample11==========\n",
            "Docstring: Finds parent folder of file\n",
            "\n",
            "    :param file_path: path\n",
            "    :return: Name of folder container\n",
            "[{'answer': 'get_parent_folder_name', 'question': 'What is the name of the function that finds the parent folder of a file?'}]\n",
            "========Sample12==========\n",
            "Docstring: Finds content of folder\n",
            "\n",
            "    :param path: directory to get list of files and folders\n",
            "    :param include_hidden: True iff include hidden files in list\n",
            "    :return: List of paths in given directory\n",
            "[{'answer': 'ls_dir', 'question': 'What is the name of the folder that finds the contents of?'}]\n",
            "========Sample13==========\n",
            "Docstring: Finds content of folder recursively\n",
            "\n",
            "    :param path: directory to get list of files and folders\n",
            "    :param include_hidden: True iff include hidden files in list\n",
            "    :return: List of paths in given directory recursively\n",
            "[{'answer': 'ls_recurse', 'question': 'What is the name of the function that finds content of a folder recursively?'}]\n",
            "========Sample14==========\n",
            "Docstring: Finds content of folder (recursively)\n",
            "\n",
            "    :param path: directory to get list of files and folders\n",
            "    :param recurse: True iff recurse into subdirectories or not\n",
            "    :param include_hidden: True iff include hidden files in list\n",
            "    :return: List of paths in given directory recursively\n",
            "[{'answer': 'list_content', 'question': 'What is the name of the file that finds content?'}]\n",
            "========Sample15==========\n",
            "Docstring: Checks if file path is russian\n",
            "\n",
            "        :return: True iff document has a russian name\n",
            "[{'answer': 'FileSystem.is_russian', 'question': 'What is the name of the filesystem?'}]\n",
            "========Sample16==========\n",
            "Docstring: Renames to new path\n",
            "\n",
            "        :param new_path: new path to use\n",
            "[{'answer': 'FileSystem.rename', 'question': 'What is the name of the file system that renames to a new path?'}]\n",
            "========Sample17==========\n",
            "Docstring: This waits until the whole chain of callback methods triggered by\n",
            "        \"trigger_connection_to_rabbit_etc()\" has finished, and then starts \n",
            "        waiting for publications.\n",
            "        This is done by starting the ioloop.\n",
            "\n",
            "        Note: In the pika usage example, these things are both called inside the run()\n",
            "        method, so I wonder if this check-and-wait here is necessary. Maybe not.\n",
            "        But the usage example does not implement a Thread, so it probably blocks during\n",
            "        the opening of the connection. Here, as it is a different thread, the run()\n",
            "        might get called before the __init__ has finished? I'd rather stay on the\n",
            "        safe side, as my experience of threading in Python is limited.\n",
            "[{'answer': 'ConnectionBuilder.__start_waiting_for_events', 'question': 'What is the name of the method that waits until the whole chain of callback methods has finished?'}]\n",
            "========Sample18==========\n",
            "Docstring: Sets the constructor for the component type this label is to \n",
            "        represent\n",
            "\n",
            "        :param factoryclass: a class that, when called, results in an instance of the desired class\n",
            "        :type factoryclass: callable\n",
            "[{'answer': 'DragLabel.setClass', 'question': 'What is the name of the class that sets the constructor for the component type this label is to represent?'}]\n",
            "========Sample19==========\n",
            "Docstring: Determines if a drag is taking place, and initiates it\n",
            "[{'answer': 'DragLabel.mouseMoveEvent', 'question': 'What is the name of the event that determines if a drag is taking place?'}]\n",
            "========Sample20==========\n",
            "Docstring: Enters all the metadata into a database\n",
            "[{'answer': 'Database.database', 'question': 'What is the name of the file that enters all the metadata into a database?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zipped10 = zip(train_data[:5]['func_name'], train_data[:10]['func_documentation_string'])\n",
        "\n",
        "generator = finetuned_t5(list(zipped10))\n",
        "print(generator)\n",
        "\n",
        "for item in generator:\n",
        "    ans, ques = item['answer'], item['question']\n",
        "    print(f\"========Sample==========\")\n",
        "    print(f\"Question: {ques}\")\n",
        "    print(f\"Answer: {ans}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K2Wd_0ByUjFb",
        "outputId": "20ef0842-5f3c-41be-f26b-7e5c641c8001"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object QGPipeline.__call__.<locals>.batch_generator at 0x7e667d4d5d40>\n",
            "========Sample==========\n",
            "Question: What is the function that shows a slug with level?\n",
            "Answer: show_slug_with_level\n",
            "\n",
            "========Sample==========\n",
            "Question: What is the function that shows the last 10 revisions of a page?\n",
            "Answer: show_revisions\n",
            "\n",
            "========Sample==========\n",
            "Question: What is the name of the function that parses the imageplaceholder template tag?\n",
            "Answer: do_videoplaceholder\n",
            "\n",
            "========Sample==========\n",
            "Question: What is the function do_get_pages_with_tag?\n",
            "Answer: do_get_pages_with_tag\n",
            "\n",
            "========Sample==========\n",
            "Question: What is the function that parses the XML run statistics file?\n",
            "Answer: Metadata.parserunstats\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Processor"
      ],
      "metadata": {
        "id": "IPREFUqlXIbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "from datasets import Dataset, DatasetDict\n",
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "class DocstringDatasetProcessor:\n",
        "    def __init__(self,\n",
        "                 hf_dataset_name: str,\n",
        "                 batch_size: int = 1000,\n",
        "                 save_locally: bool = False,\n",
        "                 local_cache_dir: str = \"./cache\",\n",
        "                 private_repo: bool = False):\n",
        "\n",
        "        self.hf_dataset_name = hf_dataset_name\n",
        "        self.batch_size = batch_size\n",
        "        self.private_repo = private_repo\n",
        "        self.local_cache_dir = Path(local_cache_dir)\n",
        "        self.local_cache_dir.mkdir(exist_ok=True)\n",
        "        self.save_locally = save_locally\n",
        "\n",
        "        self.processed_count = 0\n",
        "        self.failed_count = 0\n",
        "        self.all_geneated_data = []\n",
        "\n",
        "        self.hf_api = HfApi()\n",
        "\n",
        "    def authenticate_hf(self):\n",
        "        \"\"\"Authenticate with Hugging Face Hub\"\"\"\n",
        "        try:\n",
        "            login()\n",
        "            print(\"Successfully authenticated\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error authenticating with Hugging Face Hub: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def process_batch(self, batch_data: List[Tuple[str, str]], pipeline: QGPipeline, batch_id: int) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process a batch of (func_name, docstring) tuples with individual error handling\"\"\"\n",
        "        batch_results = []\n",
        "        batch_success_count = 0\n",
        "        batch_failure_count = 0\n",
        "\n",
        "        # Process the entire batch through pipeline\n",
        "        try:\n",
        "            # Pipeline returns a generator that yields results with error info\n",
        "            for result in pipeline(batch_data):\n",
        "                if result['success']:\n",
        "                    # Successfully processed item\n",
        "                    batch_results.append({\n",
        "                        'function_name': result['function_name'],\n",
        "                        'docstring': result['docstring'],\n",
        "                        'question': result['result']['question'],\n",
        "                        'answer': result['result']['answer']\n",
        "                    })\n",
        "                    batch_success_count += 1\n",
        "                else:\n",
        "                    # Failed to process item\n",
        "                    self.logger.warning(\n",
        "                        f\"Failed to process {result['function_name']}: {result['error']}\"\n",
        "                    )\n",
        "                    batch_failure_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catastrophic failure - entire batch failed\n",
        "            self.logger.error(f\"Catastrophic batch failure {batch_id}: {e}\")\n",
        "            batch_failure_count = len(batch_data)\n",
        "            batch_success_count = 0\n",
        "\n",
        "        # Update global counters\n",
        "        self.processed_count += batch_success_count\n",
        "        self.failed_count += batch_failure_count\n",
        "\n",
        "        # Log batch statistics\n",
        "        self.logger.info(\n",
        "            f\"Batch {batch_id}: {batch_success_count} successful, \"\n",
        "            f\"{batch_failure_count} failed out of {len(batch_data)} items\"\n",
        "        )\n",
        "\n",
        "        # Save locally if enabled\n",
        "        if self.save_locally and batch_results:\n",
        "            self._save_batch_locally(batch_results, batch_id)\n",
        "\n",
        "        return batch_results\n",
        "\n",
        "    def _save_batch_locally(self, batch_results: List[Dict], batch_id: int):\n",
        "        batch_file = self.local_cache_dir / f\"batch_{batch_id}.jsonl\"\n",
        "        with open(batch_file, 'w') as f:\n",
        "            for item in batch_results:\n",
        "                json.dump(item, f)\n",
        "                f.write('\\n')\n",
        "\n",
        "    def process_full_dataset(self, train_data, pipeline, start_idx: int = 0):\n",
        "        \"\"\"Process the entire data set and upload to hugging face\"\"\"\n",
        "\n",
        "        print(f\"Starting processing of {len(train_data)} items from index {start_idx}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_start in tqdm(range(start_idx, len(train_data), self.batch_size),\n",
        "                                desc=\"Processing batches\"):\n",
        "            batch_end = min(batch_start + self.batch_size, len(train_data))\n",
        "            batch_data = train_data[batch_start:batch_end]\n",
        "            batch_id = batch_start // self.batch_size\n",
        "\n",
        "            batch_results = self.process_batch(batch_data, pipeline, batch_id)\n",
        "            self.all_geneated_data.extend(batch_results)\n",
        "\n",
        "            # print progress\n",
        "            if batch_id % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = self.processed_count / elapsed if elapsed > 0 else 0\n",
        "                print(f\"Processed {self.processed_count} items in {elapsed:.2f} seconds. Rate: {rate:.2f} items/sec\")\n",
        "\n",
        "            #final statistics\n",
        "            total_time = time.time() - start_time\n",
        "            print(f\"Processed {self.processed_count} items in {total_time:.2f} seconds. Rate: {self.processed_count / total_time:.2f} items/sec\")\n",
        "\n",
        "            self._upload_to_hf()\n",
        "            return self.all_geneated_data\n",
        "\n",
        "    def _upload_to_hf(self):\n",
        "        if self.private_repo:\n",
        "            self.authenticate_hf()\n",
        "\n",
        "    def _upload_to_hf(self):\n",
        "        try:\n",
        "            print(\"Creating Hugging Face dataset\")\n",
        "\n",
        "            dataset = Dataset.from_list(self.all_generated_data)\n",
        "\n",
        "            #todo consider a test/train split\n",
        "            dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "            dataset_dict = DatasetDict({\n",
        "                'train': dataset['train'],\n",
        "                'validation': dataset['test']\n",
        "            })\n",
        "\n",
        "            #notice this might introduce unneccessary inefficiencies\n",
        "            dataset_dict = dataset_dict.map(\n",
        "                lambda x: {\n",
        "                    **x,\n",
        "                    'id': f\"{x['function_name']}_{hash(x['docstring']) % 10000}\"\n",
        "                }\n",
        "            )\n",
        "\n",
        "            print(f\"Uploading dataset to {self.hf_dataset_name}...\")\n",
        "\n",
        "            dataset_dict.push_to_hub(\n",
        "                self.hf_dataset_name,\n",
        "                private=self.private_repo,\n",
        "                commit_message=f\"Add {len(self.all_generated_data)} docstring-question pairs\"\n",
        "            )\n",
        "\n",
        "            print(f\"Successfully uploaded dataset to https://huggingface.co/datasets/{self.hf_dataset_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading to Hugging Face: {e}\")\n",
        "            if self.save_locally:\n",
        "                print(\"Data is available locally in cached directory\")\n",
        "            raise\n",
        "\n",
        "    def load_from_hf(self):\n",
        "        \"\"\"Load the dataset from Hugging Face\"\"\"\n",
        "\n",
        "        from datasets import load_dataset\n",
        "\n",
        "        try:\n",
        "            dataset = load_dataset(self.hf_dataset_name)\n",
        "            print(f\"Successfully loaded dataset from {self.hf_dataset_name}\")\n",
        "            return dataset\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset from Hugging Face: {e}\")\n",
        "            raise\n",
        "\n",
        "    #TODO: resume processing from local cache file\n",
        "    #TODO: upload from colab cache to permanent file location (local or drive)\n",
        "\n"
      ],
      "metadata": {
        "id": "_M83XuxNZ2tF"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NK5CdGjViK8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
